\nonstopmode

\documentclass[letterpaper,12pt,titlepage]{article}
\usepackage{amsthm,amssymb,mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{bm}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,patterns,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[L]{K.\ Okkelberg}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.5pt}

\newcommand*\dif{\mathop{}\!\mathrm{d}}
\newcommand{\trans}{^\text{T}}
\newcommand{\herm}{^\text{H}}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\sign}{sign}
\let\Pr\relax
\DeclareMathOperator{\Pr}{P}
\newcommand*\pder[2]{\frac{\partial #1}{\partial #2}}
\newcommand*\R{\mathbb{R}}

\tikzstyle{line} = [draw,>=latex]
\tikzstyle{dot} = [circle,fill=black,inner sep=0pt,minimum size=4pt]

\begin{document}

\title{ECE 6553: Homework \#1}
\author{Klaus Okkelberg}
\date{January 26, 2017}
\maketitle

\setlist{listparindent=\parindent}

\begin{enumerate}[leftmargin=0pt]

\item \begin{enumerate}
  \item Consider vectors $\epsilon v\in\R^m$, $\epsilon\in\R$, $\Vert v\Vert=1$. Notice that $\epsilon$ is the magnitude and $v$ is the direction of the vector. The size of the growth of $g(u)$ in the direction of $\epsilon v$ is given by
    \begin{align}
      g(u+\epsilon v) - g(u) &= g(u) + \epsilon\pder{g}{u}(u)v + o(\epsilon) - g(u) \\
                             &= \epsilon\pder{g}{u}(u)v + o(\epsilon) \\
                             &= \epsilon \pder{g\trans}{u}(u) \cdot v + o(\epsilon) \\
                             &= \epsilon \nabla g(u) \cdot v + o(\epsilon) \\
                             &= \epsilon \Vert\nabla g(u)\Vert \, \Vert v\Vert \cos\theta + o(\epsilon) \\
                             &= \epsilon \Vert\nabla g(u)\Vert \cos\theta + o(\epsilon),
    \end{align}
    where $\theta$ is the angle between $\nabla g(u)$ and $v$. We can see that the amount of increase is maximized when $\theta=0$, i.e.\ $v$ points along $\nabla g(u)$. Therefore, $g$ grows the most in the direction of $\nabla g(u)$. \qed
    
  %\item The directional derivative of $g:\R^m\to\R$ at $u\in\R^m$ along $v\in\R^m$ is given by
  %  \begin{align}
  %    \delta g(u;v) &= \lim_{h\to0} \frac{g(u+hv)-g(u)}{h} \\
  %                  &= \lim_{h\to0} \frac{g(u)+h\pder{g}{u}(u)v-g(u)+o(h)}{h} \\
  %                  &= \pder{g}{u}(u) v \\
  %                  &= pder{g\trans}{u}(u) \cdot v \\
  %                  &= \nabla g(u) \cdot v \\
  %                  &= \Vert\nabla g(u)\Vert \Vert v\Vert \cos \theta,
  %  \end{align}
  %  where $\theta$ is the angle between $\nabla g(u)$ and $v$. This directional derivative indicates how much $g$ grows at the point $u$ in the direction of $v$. Notice that it is maximized when $\nabla g(u)$ and $v$ are aligned, i.e.\ $g$ grows the most along $v=\nabla g(u)$.

  \item Let $u=r(t)$, $t\in\R$, be the curve that satisfies the constraint $h(u)=0$, i.e.\ $h(r(t))=0$ $\forall t$. Taking the derivative of the constraint using the chain rule,
    \begin{align}
      \frac{\dif}{\dif t} h(r(t)) &= \pder{h(r(t))}{r(t)} \frac{\dif r(t)}{\dif t} \\
                                  &= \pder{h(u)}{u} r'(t) \\
                                  &= \nabla h(u) \cdot r'(t) = 0
    \end{align}
    Notice that the derivative of the curve $r'(t)$ is the tangent plane to the constraint set at $u$. Since the dot product of the gradient and $r'(t)$ is 0, the gradient $\nabla g(u)$ is orthogonal to the tangent plane to the constraint set at $u$. \qed
  \end{enumerate}

\item The Lagrangian is
  \[ L = (u_1-2)^2 + 2(u_2-1)^2 + \lambda_1(u_1+4u_2-3) + \lambda_2(u_2-u_1) \]
  The FONCs are
  \begin{align}
    \pder{L}{u_1} = 2(u_1-2) + \lambda_1 - \lambda_2 &= 0 \\
    \pder{L}{u_2} = 4(u_2-1) + 4\lambda_1 + \lambda_2 &= 0\\
    \lambda_1(u_1+4u_2-3) &= 0 \\
    \lambda_2(u_2-u_1) &= 0 \\
    u_1+4u_2-3 &\le 0 \\
    u_2-u_1 &\le 0 \\
    \lambda_1 &\ge 0 \\
    \lambda_2 &\ge 0
  \end{align}
  Consider the following combinations in which the constraints can be active/inactive:
  \begin{enumerate}
  \item $\lambda_1=\lambda_2=0$
    \[ \begin{cases}
        2(u_1-2) = 0 \\
        4(u_2-1) = 0
      \end{cases} \Longrightarrow
      \begin{cases}
        u_1 = 2 \\
        u_2 = 1
      \end{cases} \]
    Since $u_1+4u_2 \nleq 3$, this solution is not feasible.
  \item $u_1+4u_2-3 = 0$, $\lambda_2=0$
    \[ \begin{cases}
        2(u_1-2) + \lambda_1 = 0 \\
        4(u_2-1) + 4\lambda_1 = 0 \\
        u_1 + 4u_2 = 3
      \end{cases} \Longrightarrow
      \begin{bmatrix} u_1 \\ u_2 \\ \lambda_1 \end{bmatrix} =
      \begin{bmatrix}
        2 & 0 & 1 \\
        0 & 4 & 4 \\
        1 & 4 & 0
      \end{bmatrix}^{-1}
      \begin{bmatrix}
        4 \\ 4 \\ 3
      \end{bmatrix} =
      \begin{bmatrix}
        5/3 \\ 1/3 \\ 1
      \end{bmatrix}
    \]
    We can verify that the constraints hold:
    \begin{align}
      u_1 + 4u_2 &\le 3 \\
      u_1 &\ge u_2
    \end{align}
    Therefore, $u_1=5/3$, $u_2=1/3$ is a local minimizer.
  \item $\lambda_1=0$, $u_1=u_2$
    \[ \begin{cases}
        2(u_1-2) - \lambda_2 = 0 \\
        4(u_2-1) + \lambda_2 = 0 \\
        u_1 = u_2
      \end{cases} \Longrightarrow
      \begin{cases}
        u_1 = u_2 = 4/3 \\
        \lambda_2 = -4/3
      \end{cases}
    \]
    Since $\lambda_2<0$, this solution is not feasible.
  \item $u_1+4u_2-3=0$, $u_1=u_2$
    \[ \begin{cases}
        2(u_1-2) + \lambda_1 - \lambda_2 = 0 \\
        4(u_2-1) + 4\lambda_1 + \lambda_2 = 0 \\
        u_1 + 4u_2 = 3 \\
        u_1 = u_2
      \end{cases} \Longrightarrow
      \begin{cases}
        u_1 = u_2 = 3/5 \\
        \lambda_1 = \\
        \lambda_2 = 
      \end{cases}
    \]
    this solution is not feasible.

    Since the optimization problem only has one feasible solution, the global minimum is given by
    \begin{align}
      u^*_1 &= 5/3 \\
      u^*_2 &= 1/3
    \end{align}
  \end{enumerate}

\item \begin{enumerate}
  \item The minimization problem is
    \begin{gather}
      \min \alpha u_1^2 + \beta u_2^2 \\
      \text{s.t. } u_1 + u_2 = q
    \end{gather}
    The Lagrangian is
    \[ L = \alpha u_1^2 + \beta u_2^2 + \lambda(u_1+u_2-q) \]
    The FONCs are
    \begin{align}
      \pder{L}{u_1} = 2\alpha u_1 + \lambda &= 0 \\
      \pder{L}{u_2} = 2\beta u_2 + \lambda &= 0 \\
      \pder{L}{\lambda} = u_1+u_2-q &= 0
    \end{align}
    From the first two conditions, $\alpha u_1 = \beta u_2$. Substituting this into the third condition gives
    \begin{align}
      u_1 &= \frac{\beta}{\alpha+\beta} q \\
      u_2 &= \frac{\alpha}{\alpha+\beta} q
    \end{align}
  \item It is clear that the highest cost is achieved by only shipping through the more expensive option, i.e.
    \[
      \begin{cases}
        \max \alpha u_1^2 + \beta u_2^2 \\
        \text{s.t. } u_1 + u_2 = q
      \end{cases}
      \Longrightarrow
      \begin{cases}
        u^*_i = q, & i = \arg\max(\alpha,\beta) \\
        u^*_j = 0, & j = \arg\min(\alpha,\beta)
      \end{cases}
    \]
    This gives a total cost of $\max(\alpha,\beta)q^2$. The cost of the minimizers from the previous part is
    \[ \alpha \left(\frac{\beta}{\alpha+\beta} q\right)^2 + \beta \left(\frac{\alpha}{\alpha+\beta} q\right)^2 = \frac{\alpha\beta}{\alpha+\beta} q^2 < \min(\alpha,\beta) q^2 \le \max(\alpha,\beta) q^2 \]
    Therefore, the combination found in the previous part is not the worst combination and must be the best combination instead.
  \end{enumerate}

\item By convexity of $g$, for all $u\in\R^m$ and $\alpha\in[0,1]$,
  \begin{gather}
    g(\alpha u + (1-\alpha)u^*) \le \alpha g(u) + (1-\alpha)g(u^*) \\
    g(\alpha u + (1-\alpha)u^*) \le g(u^*) + \alpha (g(u)-g(u^*)) \\
    g(u)-g(u^*) \ge \frac{g(\alpha u + (1-\alpha)u^*) - g(u^*)}{\alpha}
  \end{gather}
  Taking the limit as $\alpha\to0$,
  \begin{align}
    g(u)-g(u^*) &\ge \lim_{\alpha\to0} \frac{g(\alpha u + (1-\alpha)u^*) - g(u^*)}{\alpha} \\
                &= \left[ \frac{\dif}{\dif\alpha} \Big\{ g(\alpha u + (1-\alpha)u^*) - g(u^*) \Big\} \right]_{\alpha=0} \\
                &= \pder{g}{u}(u^*) \left[ \frac{\dif}{\dif\alpha} (\alpha u + (1-\alpha)u^*) \right]_{\alpha=0} \\
                &= \pder{g}{u}(u^*) (u - u^*) = 0 \\
    g(u) &\ge g(u^*),
  \end{align}
  where L'H\^opital's rule was used to calculate the limit.
  Since $g(u^*)\le g(u)$ $\forall u\in\R^m$, $u^*$ is the global minimum to $g$.

%\item Assume that $u^*$ is not the global minimum to $g$. Therefore, there exists $u_0\ne u^*$ such that $g(u_0)<g(u^*)$. By convexity of $g$, for $0<\alpha\le 1$,
%  \[
%    g(\alpha u_0 + (1-\alpha)u^*) \le \alpha g(u_0) + (1-\alpha)g(u^*) < \alpha g(u^*) + (1-\alpha)g(u^*) = g(u^*)
%  \]
%  Since 

\item

\item
  
\end{enumerate}

\end{document}
